{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction\n",
    "\n",
    "**Author**: Arnav Sharma - arnav.sharma2264@gmail.com - github.com/ArnavSharma2\n",
    "**Date**: 2025-08-30 \n",
    "**Description**: WIP\n",
    "\n",
    "## Table of Contents\n",
    "1. [Project Setup](#1-project-setup)\n",
    "2. [Data Loading](#2-data-loading)\n",
    "3. [Exploratory Data Analysis (EDA)](#3-exploratory-data-analysis-eda)\n",
    "4. [Data Preprocessing](#4-data-preprocessing)\n",
    "5. [Feature Engineering](#5-feature-engineering)\n",
    "6. [Model Training](#6-model-training)\n",
    "7. [AutoML](#7-automl)\n",
    "8. [Deep Learning](#8-deep-learning)\n",
    "9. [Model Evaluation](#9-model-evaluation)\n",
    "10. [Hyperparameter Tuning](#10-hyperparameter-tuning)\n",
    "11. [Model Interpretation](#11-model-interpretation)\n",
    "12. [Model Deployment](#12-model-deployment)\n",
    "13. [Conclusion and Next Steps](#13-conclusion-and-next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Project Setup\n",
    "Install and import necessary libraries, set random seeds for reproducibility, and configure project settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment to install)\n",
    "# !pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "# from flaml import AutoML\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, Dropout\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import shap\n",
    "# from skopt import BayesSearchCV\n",
    "from joblib import dump, load \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "#gender, PaymentMethod, MonthlyCharges, Dependents\n",
    "PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(PATH, \"Dataset/stock-price-prediction-challenge/train/stocks/AAPL.csv\")\n",
    "\n",
    "# Define project-specific variables\n",
    "# DATA_PATH = \"/Users/aps/Desktop/ML-DL-Projects/01-housepriceprediction/Dataset/HousePrices.csv\"  # Update with your dataset path\n",
    "TARGET_COLUMN = \"Returns\"  # Update with your target column name\n",
    "# DROP_COLUMNS = ['yr_renovated', 'yr_built', 'condition', 'sqft_lot', 'country', 'waterfront', 'floors', 'date','street'] # list of columns to drop\n",
    "DROP_COLUMNS = ['Adjusted', 'Ticker', 'Date']\n",
    "HANDLE_OUTLIERS = ['Returns', 'Volume']\n",
    "TASK_TYPE = \"regression\"  # Options: \"classification\" or \"regression\"\n",
    "MODEL_SAVE_PATH = \"model.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "Load the dataset and perform initial checks for data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load dataset from a given file path.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the dataset (CSV, Excel, etc.)\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Loaded dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            data = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx'):\n",
    "            data = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format\")\n",
    "        \n",
    "        print(\"Data loaded successfully!\")\n",
    "        print(f\"Shape: {data.shape}\")\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(data.head())\n",
    "        print(\"\\nData Info:\")\n",
    "        print(data.info())\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "# Load the dataset\n",
    "df = load_data(DATA_PATH)\n",
    "if df is None:\n",
    "    raise SystemExit(\"Data loading failed. Exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "Understand the dataset through visualizations and statistical summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df, target_column):\n",
    "    \"\"\"\n",
    "    Perform exploratory data analysis on the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataset\n",
    "    target_column (str): Name of the target column\n",
    "    \"\"\"\n",
    "    # Summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(df.describe(include='all'))\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Grab all the numerical cols\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    # Distribution of target variable\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if TASK_TYPE == \"classification\":\n",
    "        sns.countplot(x=target_column, data=df)\n",
    "        plt.title(f\"Distribution of {target_column}\")\n",
    "    else:\n",
    "        # sns.histplot(df[target_column], kde=True)\n",
    "        for col in num_cols:\n",
    "            sns.boxplot(x=df[col])\n",
    "            plt.title(f\"Distribution of {col} (Regression)\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Correlation matrix (for numeric columns)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Pairplot for numeric features (optional, for smaller datasets)\n",
    "    # sns.pairplot(df[numeric_cols])\n",
    "    # plt.show()\n",
    "\n",
    "# Perform EDA\n",
    "perform_eda(df, TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "Clean the data and prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_with_ohe(ohe_df, colname):\n",
    "    ohe = OneHotEncoder(drop=None, sparse_output=False)\n",
    "    ohe1 = ohe.fit_transform(ohe_df[[colname]])\n",
    "    ohe2 = pd.DataFrame(ohe1, columns=ohe.get_feature_names_out([colname]), index=ohe_df.index)\n",
    "    transformed = pd.concat([ohe_df.drop(colname, axis=1), ohe2], axis=1)\n",
    "    transformed.head()\n",
    "    return transformed\n",
    "\n",
    "def transform_with_le(df, col):\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "def remove_any_outliers(outlier_df, colname):\n",
    "    # This line of code keeps only the middle 98% of price values and removes the most extreme 2% (lowest 1% + highest 1%) from your dataset.\n",
    "    # Keeps only the rows where price is greater than the 1st percentile and less than the 99th percentile.\n",
    "    # In other words, it removes the extreme 1% lowest and 1% highest values.\n",
    "    high = 0.99\n",
    "    low = 1-high\n",
    "\n",
    "    removed_outliers = outlier_df[(outlier_df[colname] > outlier_df[colname].quantile(low)) & \n",
    "                        (outlier_df[colname] < outlier_df[colname].quantile(high))]\n",
    "    removed_outliers.columns\n",
    "    # Plot the Before and After graph\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    # Before\n",
    "    sns.boxplot(x=outlier_df[colname], ax=axes[0])\n",
    "    axes[0].set_title(\"Before Removing Outliers\")\n",
    "    # After\n",
    "    sns.boxplot(x=removed_outliers[colname], ax=axes[1])\n",
    "    axes[1].set_title(\"After Removing Outliers\")\n",
    "    plt.show()\n",
    "    return removed_outliers\n",
    "\n",
    "\n",
    "def preprocess_data(df, target_column):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset (handle missing values, encode categorical variables, etc.).\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataset\n",
    "    target_column (str): Name of the target column\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Preprocessed dataset\n",
    "    \"\"\"\n",
    "    # # Separate features and target\n",
    "    # X = df.drop(columns=[target_column])\n",
    "    # y = df[target_column]\n",
    "    # removing unimportant features\n",
    "    processed = df.drop(DROP_COLUMNS, axis=1)\n",
    "    labelencoding = ['']\n",
    "    onehotencoding = ['']\n",
    "    # Handle missing values\n",
    "    numeric_cols = processed.select_dtypes(include=[np.number]).columns\n",
    "    categorical_cols = processed.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    processed['Returns'] = (processed['Close']-processed['Open'])/processed['Open']\n",
    "\n",
    "    \n",
    "    # Impute numeric columns with median\n",
    "    for col in numeric_cols:\n",
    "        processed[col].fillna(processed[col].median(), inplace=True)\n",
    "    \n",
    "    # Impute categorical columns with mode, Grab the final DF before OHE so we can do SHAP\n",
    "    # for col in categorical_cols:\n",
    "    #     SHAP_DF = processed[col].fillna(processed[col].mode()[0], inplace=True)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    # le = LabelEncoder()\n",
    "    # for col in categorical_cols:\n",
    "    #     X[col] = le.fit_transform(X[col])\n",
    "\n",
    "    # for col in labelencoding:\n",
    "    #     processed = transform_with_le(processed, col)\n",
    "\n",
    "    # Scale the numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    numeric_cols = processed.select_dtypes(include=[np.number]).columns\n",
    "    processed[numeric_cols] = scaler.fit_transform(processed[numeric_cols])\n",
    "    \n",
    "    # Handle any outlier needs\n",
    "    for col in HANDLE_OUTLIERS:\n",
    "        processed = remove_any_outliers(processed,col)\n",
    "    # split into X and Y DF\n",
    "    X = processed.drop(target_column,axis=1)\n",
    "    y = processed[target_column]\n",
    "\n",
    "    print(\"Data preprocessing completed!\")\n",
    "    #return X, y, scaler\n",
    "    return X, y\n",
    "\n",
    "# Preprocess data\n",
    "# X, y, scaler = preprocess_data(df, TARGET_COLUMN)\n",
    "X, y = preprocess_data(df, TARGET_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "Create new features and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def feature_engineering(X):\n",
    "#     # \"\"\"\n",
    "#     # Perform feature engineering (e.g., creating new features, scaling).\n",
    "    \n",
    "#     # Parameters:\n",
    "#     # X (pandas.DataFrame): Feature dataset\n",
    "    \n",
    "#     # Returns:\n",
    "#     # pandas.DataFrame: Engineered feature dataset\n",
    "#     # \"\"\"\n",
    "#     # # Example: Create interaction features (customize as needed)\n",
    "#     # if 'feature1' in X.columns and 'feature2' in X.columns:\n",
    "#     #     X['feature1_feature2_interaction'] = X['feature1'] * X['feature2']\n",
    "    \n",
    "#     # Scale numeric features\n",
    "#     scaler = StandardScaler()\n",
    "#     numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "#     numeric_cols\n",
    "#     X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "    \n",
    "#     # print(\"Feature engineering completed!\")\n",
    "#     return X, scaler\n",
    "\n",
    "#     # Implementing One Hot Encoder on City and Statezip\n",
    "#     ohe = OneHotEncoder(drop=None, sparse_output=False)\n",
    "#     city_ohe = ohe.fit_transform(d[['city']])\n",
    "\n",
    "#     city_df = pd.DataFrame(city_ohe, columns=ohe.get_feature_names_out(['city']), index=X.index)\n",
    "\n",
    "#     return pd.concat([X.drop('city', axis=1), city_df], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# # Perform feature engineering\n",
    "# X = feature_engineering(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training\n",
    "Split data and train multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_sequences(X, y, time_steps=60):\n",
    "#     \"\"\"\n",
    "#     Reshapes 2D data into 3D sequences for LSTM models.\n",
    "#     \"\"\"\n",
    "#     X_seq, y_seq = [], []\n",
    "#     for i in range(len(X) - time_steps):\n",
    "#         # Take a sequence of 'time_steps' length\n",
    "#         X_seq.append(X[i:(i + time_steps)])\n",
    "#         # The corresponding label is the value that comes immediately after the sequence\n",
    "#         y_seq.append(y[i + time_steps])\n",
    "#     return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "def train_models(X, y):\n",
    "    \"\"\"\n",
    "    Train multiple machine learning models, AutoML, and a deep learning model.\n",
    "    \n",
    "    Parameters:\n",
    "    X (pandas.DataFrame): Features\n",
    "    y (numpy.array): Target\n",
    "    \n",
    "    Returns:\n",
    "    dict: Trained models (including AutoML and Deep Learning)\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Initialize models based on task type\n",
    "    models = {}\n",
    "    if TASK_TYPE == \"classification\":\n",
    "        models['Logistic Regression'] = LogisticRegression(random_state=42)\n",
    "        models['Random Forest'] = RandomForestClassifier(random_state=42)\n",
    "        models['XGBoost'] = XGBClassifier(random_state=42)\n",
    "    else:\n",
    "        models['Linear Regression'] = LinearRegression()\n",
    "        models['Random Forest'] = RandomForestRegressor(random_state=42)\n",
    "        models['XGBoost'] = XGBRegressor(random_state=42)\n",
    "\n",
    "    # Train manual models\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        print(f\"{name} trained successfully!\")\n",
    "    \n",
    "        # Deep Learning Model (Simple Neural Network)\n",
    "    X_copy = X_train\n",
    "    y_copy = y_train\n",
    "    dl_model = Sequential()\n",
    "    dl_model.add(Dense(64, activation='relu', input_shape=(X_copy.shape[1],)))\n",
    "    dl_model.add(Dropout(0.2))\n",
    "    dl_model.add(Dense(32, activation='relu'))\n",
    "    dl_model.add(Dropout(0.2))\n",
    "    if TASK_TYPE == \"classification\":\n",
    "        dl_model.add(Dense(1, activation='sigmoid'))\n",
    "        dl_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        dl_model.add(Dense(1))\n",
    "        dl_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train Deep Learning model\n",
    "    dl_model.fit(X_copy, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.2)\n",
    "    models['Deep Learning'] = dl_model\n",
    "    print(\"Deep Learning model trained successfully!\")\n",
    "\n",
    "    # X_train_seq, y_train_seq = create_sequences(X_train, y_train)\n",
    "    # # LSTM Model\n",
    "    # time_steps = 60\n",
    "    # lstm_model = Sequential()\n",
    "    # lstm_model.add(LSTM(50, activation='relu', input_shape=(time_steps, X_train_seq.shape[1]), return_sequences=False))\n",
    "    # lstm_model.add(Dropout(0.2))\n",
    "    # lstm_model.add(Dense(25, activation='relu'))\n",
    "    # lstm_model.add(Dropout(0.2))\n",
    "    # if TASK_TYPE == \"classification\":\n",
    "    #     lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "    #     lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    # else:\n",
    "    #     lstm_model.add(Dense(1))\n",
    "    #     lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # # Train LSTM model\n",
    "    # lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0, validation_split=0.2)\n",
    "    # models['LSTM'] = lstm_model\n",
    "    # print(\"LSTM model trained successfully!\")\n",
    "    \n",
    "    # Run AutoML\n",
    "    #automl_model = run_automl(X_train, y_train, TASK_TYPE, time_budget=60)\n",
    "    #models['AutoML'] = automl_model\n",
    "\n",
    "    return models, X_train, X_test, y_train, y_test\n",
    "\n",
    "# Train models\n",
    "models, X_train, X_test, y_train, y_test = train_models(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AutoML\n",
    "Use an AutoML framework (FLAML) to automatically select and tune the best model for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml import AutoML\n",
    "\n",
    "def run_automl(X_train, y_train, task_type, time_budget=60):\n",
    "    \"\"\"\n",
    "    Run AutoML using FLAML to automatically select and tune the best model.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (pandas.DataFrame): Training features\n",
    "    y_train (numpy.array): Training target\n",
    "    task_type (str): 'classification' or 'regression'\n",
    "    time_budget (int): Time budget in seconds for AutoML (default: 60)\n",
    "    \n",
    "    Returns:\n",
    "    model: Best AutoML model\n",
    "    \"\"\"\n",
    "    # time_budget = 300\n",
    "    automl = AutoML()\n",
    "    automl_settings = {\n",
    "        \"time_budget\": time_budget,  # Time budget in seconds\n",
    "        \"metric\": \"accuracy\" if task_type == \"classification\" else \"rmse\",\n",
    "        \"task\": task_type,\n",
    "        \"log_file_name\": \"automl.log\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "    \n",
    "    automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
    "    print(f\"Best AutoML model: {automl.best_estimator}\")\n",
    "    print(f\"Best configuration: {automl.best_config}\")\n",
    "    \n",
    "    return automl\n",
    "\n",
    "# Run AutoML\n",
    "# automl_model = run_automl(X_train, y_train, TASK_TYPE, time_budget=60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation\n",
    "Evaluate models using appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_weights(mod):\n",
    "    # 3. Access and print the weights\n",
    "    weights = mod.coef_\n",
    "    intercept = mod.intercept_\n",
    "\n",
    "    print(f\"Weights (Coefficients): {weights}\")\n",
    "    print(f\"Intercept: {intercept}\")\n",
    "\n",
    "    # 4. Optional: Create a DataFrame for better readability\n",
    "    # This pairs each weight with its corresponding feature name\n",
    "    weights_df = pd.DataFrame({'Feature': X.columns, 'Weight': weights})\n",
    "    print(\"\\nModel Weights:\")\n",
    "    print(weights_df)\n",
    "\n",
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate trained models using appropriate metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    models (dict): Dictionary of trained models\n",
    "    X_test (pandas.DataFrame): Test features\n",
    "    y_test (numpy.array): Test target\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        if name == 'Deep Learning':\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred = (y_pred > 0.5).astype(int).flatten() if TASK_TYPE == \"classification\" else y_pred.flatten()\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        if TASK_TYPE == \"classification\":\n",
    "            results[name] = {\n",
    "                'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "                'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "                'F1 Score': f1_score(y_test, y_pred, average='weighted')\n",
    "            }\n",
    "        else:\n",
    "            results[name] = {\n",
    "                'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "                'R2 Score': r2_score(y_test, y_pred)\n",
    "            }\n",
    "\n",
    "    \n",
    "    # Display results\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Evaluate models\n",
    "results_df = evaluate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning\n",
    "Optimize the best-performing model using Bayesian optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_best_model(X_train, y_train, best_model_name, models):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning on the best model (excluding AutoML and Deep Learning).\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (pandas.DataFrame): Training features\n",
    "    y_train (numpy.array): Training target\n",
    "    best_model_name (str): Name of the best model\n",
    "    models (dict): Dictionary of trained models\n",
    "    \n",
    "    Returns:\n",
    "    model: Tuned model\n",
    "    \"\"\"\n",
    "    # Define hyperparameter search space (customize as needed)\n",
    "    param_space = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': (10, 200),\n",
    "            'max_depth': (3, 20),\n",
    "            'min_samples_split': (2, 10)\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'n_estimators': (10, 200),\n",
    "            'max_depth': (3, 20),\n",
    "            'learning_rate': (0.01, 0.3, 'log-uniform')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if best_model_name in param_space:\n",
    "        bayes_cv = BayesSearchCV(\n",
    "            estimator=models[best_model_name],\n",
    "            search_spaces=param_space[best_model_name],\n",
    "            n_iter=20,\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "        bayes_cv.fit(X_train, y_train)\n",
    "        print(f\"Best parameters for {best_model_name}: {bayes_cv.best_params_}\")\n",
    "        return bayes_cv.best_estimator_\n",
    "    elif best_model_name in ['AutoML', 'Deep Learning']:\n",
    "        print(f\"{best_model_name} model is already tuned or does not support BayesSearchCV. Skipping additional tuning.\")\n",
    "        return models[best_model_name]\n",
    "    else:\n",
    "        print(f\"No tuning defined for {best_model_name}\")\n",
    "        return models[best_model_name]\n",
    "\n",
    "# Select the best model (based on evaluation metrics)\n",
    "best_model_name = results_df.idxmax()['Accuracy' if TASK_TYPE == \"classification\" else 'R2 Score']\n",
    "tuned_model = tune_best_model(X_train, y_train, best_model_name, models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Interpretation\n",
    "Interpret the model using SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_model(model, X_train, model_name):\n",
    "    \"\"\"\n",
    "    Interpret the model using SHAP values.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    X_train (pandas.DataFrame): Training features\n",
    "    model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if model_name in ['Random Forest', 'XGBoost']:\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_train)\n",
    "        elif model_name == 'AutoML':\n",
    "            # Check if the AutoML model is tree-based\n",
    "            if 'rf' in model.best_estimator.lower() or 'xgboost' in model.best_estimator.lower():\n",
    "                explainer = shap.TreeExplainer(model.model)\n",
    "                shap_values = explainer.shap_values(X_train)\n",
    "            else:\n",
    "                explainer = shap.KernelExplainer(model.predict, X_train)\n",
    "                shap_values = explainer.shap_values(X_train)\n",
    "        elif model_name == 'Deep Learning':\n",
    "            explainer = shap.DeepExplainer(model, X_train)\n",
    "            shap_values = explainer.shap_values(X_train.to_numpy())\n",
    "        else:\n",
    "            explainer = shap.KernelExplainer(model.predict, X_train)\n",
    "            shap_values = explainer.shap_values(X_train)\n",
    "        \n",
    "        # Summary plot\n",
    "        shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "        plt.title(f\"Feature Importance (SHAP) - {model_name}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP interpretation failed for {model_name}: {e}\")\n",
    "\n",
    "# Interpret the best model\n",
    "interpret_model(tuned_model, X_train, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Deployment\n",
    "Save the model and create a prediction function for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_model(model, scaler, path, model_name):\n",
    "def save_model(model, path, model_name):\n",
    "    \"\"\"\n",
    "    Save the trained model and scaler.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    scaler: Fitted scaler\n",
    "    path (str): Path to save the model\n",
    "    model_name (str): Name of the model\n",
    "    \"\"\"\n",
    "    if model_name == 'Deep Learning':\n",
    "        model.save(path.replace('.pkl', '.h5'))  # Save Keras model in HDF5 format\n",
    "        # dump({'scaler': scaler}, path.replace('.pkl', '_scaler.pkl'))\n",
    "        print(f\"Deep Learning model saved to {path.replace('.pkl', '.h5')}\")\n",
    "        # print(f\"Scaler saved to {path.replace('.pkl', '_scaler.pkl')}\")\n",
    "    else:\n",
    "        # dump({'model': model, 'scaler': scaler}, path)\n",
    "        print(f\"Model and scaler saved to {path}\")\n",
    "\n",
    "def predict_new_data(model, scaler, new_data, model_name):\n",
    "    \"\"\"\n",
    "    Make predictions on new data.\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained model\n",
    "    scaler: Fitted scaler\n",
    "    new_data (pandas.DataFrame): New data for prediction\n",
    "    model_name (str): Name of the model\n",
    "    \n",
    "    Returns:\n",
    "    numpy.array: Predictions\n",
    "    \"\"\"\n",
    "    # Preprocess new data\n",
    "    numeric_cols = new_data.select_dtypes(include=[np.number]).columns\n",
    "    new_data[numeric_cols] = scaler.transform(new_data[numeric_cols])\n",
    "    \n",
    "    # Make predictions\n",
    "    if model_name == 'Deep Learning':\n",
    "        predictions = model.predict(new_data).flatten()\n",
    "        if TASK_TYPE == \"classification\":\n",
    "            predictions = (predictions > 0.5).astype(int)\n",
    "    else:\n",
    "        predictions = model.predict(new_data)\n",
    "    return predictions\n",
    "\n",
    "# Save the model\n",
    "#save_model(tuned_model, scaler, MODEL_SAVE_PATH, best_model_name)\n",
    "save_model(tuned_model, MODEL_SAVE_PATH, best_model_name)\n",
    "\n",
    "# Example: Predict on new data (replace with actual new data)\n",
    "# new_data = pd.DataFrame(...)  # Load or create new data\n",
    "# predictions = predict_new_data(tuned_model, scaler, new_data, best_model_name)\n",
    "# print(\"Predictions on new data:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Next Steps\n",
    "- **Summary**: Summarize the findings, best model performance (including AutoML and Deep Learning results), and key insights from EDA and model interpretation.\n",
    "- **Next Steps**: Consider additional feature engineering, trying other AutoML frameworks, experimenting with different neural network architectures, or deploying the model in a production environment.\n",
    "- **Monitoring**: Plan for model monitoring and retraining to handle data drift.\n",
    "\n",
    "```python\n",
    "print(\"End-to-End ML and Deep Learning Project Completed!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"End-to-End ML and Deep Learning Project Completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
